{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Electric Motor Temperature Prediction\n",
                "\n",
                "## Project Objective\n",
                "Predict the temperature of electric motor components (specifically Permanent Magnet) using sensor data to prevent overheating and improve efficiency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import necessary libraries\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "from scipy import stats\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "import math\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "%matplotlib inline\n",
                "\n",
                "# Machine Learning libraries\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.ensemble import RandomForestRegressor\n",
                "from sklearn.tree import DecisionTreeRegressor\n",
                "from sklearn.linear_model import LinearRegression\n",
                "from sklearn.metrics import mean_squared_error, r2_score\n",
                "import pickle"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Collection & Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Read the dataset\n",
                "df = pd.read_csv('pmsm_temperature_data.csv')\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Descriptive Analysis\n",
                "We'll see which particular variables contribute to the rotor temperature individually by checking their statistical significance.\n",
                "\n",
                "Descriptive analysis is to study the basic features of data with the statistical process. Here pandas have a worthy function called describe. With this described function we can understand the unique, top, and frequent values of categorical features. And we can find mean, std, min, max, and percentile values of continuous features.\n",
                "\n",
                "**df.info():**\n",
                "This function is used to display a brief introduction about the data set such as the number of rows and columns, the Data type of each column, whether the null values are present in the column or not.\n",
                "\n",
                "**df.describe():**\n",
                "This function is used to analyze the descriptive statistics of the data such as mean, median, quartile values, maximum and minimum values of each column."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.info()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Handling Missing Values\n",
                "For checking the null values, df.isnull() function is used. To sum those null values we use .sum() function to it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for null values\n",
                "df.isnull().sum()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Conclusion:**\n",
                "There are no null values in the dataset, so we can proceed without imputation."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Uni-variate Analysis\n",
                "Here we get to know about our data\n",
                "\n",
                "**Bar Graph:**\n",
                "A bar chart or bar graph is a chart or graph that presents categorical data with rectangular bars with heights or lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(15,6))\n",
                "try:\n",
                "    df['profile_id'].value_counts().sort_values().plot(kind = 'bar')\n",
                "    plt.title('Profile ID Distribution')\n",
                "except KeyError:\n",
                "    print(\"Column 'profile_id' not found in dataset. Skipping plot.\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Box Plot & Distribution Plot\n",
                "\n",
                "**Box Plot:**\n",
                "A boxplot is a standardized way of displaying the distribution of data based on a five-number summary (“minimum”, first quartile (Q1), median, third quartile (Q3), and “maximum”). It can tell you about your outliers and what their values are.\n",
                "\n",
                "**Distribution Plot:**\n",
                "The distribution plot is suitable for comparing range and distribution for groups of numerical data. Data is plotted as value points along an axis."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df.columns"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "#Plotting Distribution and Boxplot for all the features to check for skewness\n",
                "for i in df.columns:\n",
                "    if df[i].dtype != 'object':\n",
                "        sns.distplot(df[i],color='g')\n",
                "        sns.boxplot(df[i],color = 'y')\n",
                "        plt.vlines(df[i].mean(),ymin = -1,ymax = 1,color = 'r')#drawing the mean line\n",
                "        plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**Conclusion:**\n",
                "As we can see from the above plots, the mean and median for most of the plots are very close to each other. So the data seems to have low skewness for almost all variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Summary Statistics\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Handling Outliers\n",
                "With the help of a boxplot, outliers are visualized (refer to activity 3 univariate analysis). And here we are going to find the upper bound and lower bound of the Na_to_K feature with some mathematical formula.\n",
                "\n",
                "To find the upper bound we have to multiply IQR (Interquartile range) with 1.5 and add it with 3rd quantile. To find a lower bound instead of adding, subtract it with 1st quantile.\n",
                "If outliers are removed, we lose more data. It will impact model performance.\n",
                "Here removing outliers is impossible. So, the capping technique is used on outliers.\n",
                "Capping: Replacing the outliers with upper bound values.\n",
                "\n",
                "**Note:** In our Dataset all the values are in the same range, so outliers replacing is not necessary."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Handling Categorical Values\n",
                "As we can see our dataset has categorical data we must convert the categorical data to integer encoding or binary encoding.\n",
                "\n",
                "To convert the categorical features into numerical features we use encoding techniques. There are several techniques but in our project, we are using feature mapping and label encoding.\n",
                "\n",
                "**Note:** In our dataset, there is no categorical data type, so we can skip this step."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation Matrix\n",
                "plt.figure(figsize=(10, 8))\n",
                "sns.heatmap(df.corr(), annot=True, cmap='coolwarm')\n",
                "plt.title('Correlation Heatmap')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Multi-variate Analysis\n",
                "Multivariate analysis (MVA) is a Statistical procedure for the analysis of data involving more than one type of measurement or observation. It may also mean solving problems where more than one dependent variable is analyzed simultaneously with other variables.\n",
                "\n",
                "**Scatterplot:**\n",
                "A scatter plot is a type of plot or mathematical diagram using Cartesian coordinates to display values for typically two variables for a set of data.\n",
                "\n",
                "**Heat-map:**\n",
                "A heat map is a data visualization technique that shows the magnitude of a phenomenon as color in two dimensions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(20,5))\n",
                "# Compare stator components for a specific profile (e.g., profile 20)\n",
                "# Note: Assuming profile_id '20' exists, if not code will pick available one or fail gracefully\n",
                "try:\n",
                "    target_profile = 20\n",
                "    if 20 not in df['profile_id'].unique():\n",
                "         target_profile = df['profile_id'].unique()[0]\n",
                "    \n",
                "    df[df['profile_id'] == target_profile]['stator_yoke'].plot(label = 'stator yoke')\n",
                "    df[df['profile_id'] == target_profile]['stator_tooth'].plot(label = 'stator tooth')\n",
                "    df[df['profile_id'] == target_profile]['stator_winding'].plot(label = 'stator winding')\n",
                "    plt.legend()\n",
                "    plt.title(f'Stator Temperatures for Profile {target_profile}')\n",
                "except KeyError:\n",
                "    print(\"profile_id column missing or data issue.\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.figure(figsize=(14,7))\n",
                "sns.heatmap(df.corr(),annot=True)\n",
                "plt.title('Correlation Heatmap with Annotation')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(2, 4, figsize=(20, 5),sharey=True)\n",
                "# Using scatterplot as requested\n",
                "sns.scatterplot(x=df['ambient'],y=df['pm'],ax=axes[0][0])\n",
                "sns.scatterplot(x=df['coolant'],y=df['pm'],ax=axes[0][1])\n",
                "sns.scatterplot(x=df['motor_speed'],y=df['pm'],ax=axes[0][2])\n",
                "sns.scatterplot(x=df['i_d'],y=df['pm'],ax=axes[0][3])\n",
                "sns.scatterplot(x=df['u_q'],y=df['pm'],ax=axes[1][0])\n",
                "sns.scatterplot(x=df['u_d'],y=df['pm'],ax=axes[1][1])\n",
                "sns.scatterplot(x=df['i_q'],y=df['pm'],ax=axes[1][2])\n",
                "# Remove empty subplot if any\n",
                "axes[1][3].axis('off')\n",
                "\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "As we can see from the plot, all three stator components follow a similar measurement variance.\n",
                "Due to this, we can infer that there has not been much time given for the motor to cool down in between recording the sensor data.\n",
                "As profile_id is an id for each measurement session, we can remove it from any further analysis and model building."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if 'profile_id' in df.columns:\n",
                "    df.drop('profile_id',axis = 1,inplace=True)\n",
                "    print(\"Dropped profile_id\")\n",
                "else:\n",
                "    print(\"profile_id already dropped or not found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Drop unwanted features\n",
                "As we want to predict the temperatures of stator components and rotor(pm), we will drop these values from our dataset for regression. Also, torque is a quantity, which is not reliably measurable in field applications, so this feature shall be omitted in this modeling.\n",
                "\n",
                "Dropping the columns from the dataset is being concluded with the help of a scatter plot, which is available in the data analysis part."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "drop_cols = ['torque', 'stator_yoke', 'stator_tooth', 'stator_winding']\n",
                "# Check if they exist before dropping to allow re-running\n",
                "existing_cols_to_drop = [col for col in drop_cols if col in df.columns]\n",
                "if existing_cols_to_drop:\n",
                "    df.drop(existing_cols_to_drop, axis=1, inplace=True)\n",
                "    print(f\"Dropped columns: {existing_cols_to_drop}\")\n",
                "else:\n",
                "    print(\"Unwanted columns already dropped or not found\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "target = 'pm'\n",
                "features = ['ambient', 'coolant', 'u_d', 'u_q', 'motor_speed', 'i_d', 'i_q']\n",
                "\n",
                "X = df[features]\n",
                "y = df[target]"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Splitting data into train and test\n",
                "Now let’s split the Dataset into train and test sets. For splitting training and testing data, we are using the train_test_split() function from sklearn. As parameters, we are passing X, y, test_size, random_state."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Test Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Normalizing the values\n",
                "We are using minmax scaler, which is a function in preprocessing module in sklearn library."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scaling\n",
                "# Normalizing the values using MinMaxScaler as per requirement\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "scaler = MinMaxScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Model Building & Evaluation\n",
                "We will evaluate three models: Linear Regression, Decision Tree, and Random Forest to see which performs best."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Linear Regression\n",
                "lr = LinearRegression()\n",
                "lr.fit(X_train_scaled, y_train)\n",
                "y_pred_lr = lr.predict(X_test_scaled)\n",
                "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
                "rmse_lr = math.sqrt(mse_lr)\n",
                "r2_lr = r2_score(y_test, y_pred_lr)\n",
                "\n",
                "print(\"Linear Regression Results:\")\n",
                "print(f\"RMSE: {rmse_lr}\")\n",
                "print(f\"R2 Score: {r2_lr}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Decision Tree\n",
                "dt = DecisionTreeRegressor(random_state=42)\n",
                "dt.fit(X_train_scaled, y_train)\n",
                "y_pred_dt = dt.predict(X_test_scaled)\n",
                "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
                "rmse_dt = math.sqrt(mse_dt)\n",
                "r2_dt = r2_score(y_test, y_pred_dt)\n",
                "\n",
                "print(\"Decision Tree Results:\")\n",
                "print(f\"RMSE: {rmse_dt}\")\n",
                "print(f\"R2 Score: {r2_dt}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest\n",
                "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
                "rf.fit(X_train_scaled, y_train)\n",
                "y_pred_rf = rf.predict(X_test_scaled)\n",
                "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
                "rmse_rf = math.sqrt(mse_rf)\n",
                "r2_rf = r2_score(y_test, y_pred_rf)\n",
                "\n",
                "print(\"Random Forest Results:\")\n",
                "print(f\"RMSE: {rmse_rf}\")\n",
                "print(f\"R2 Score: {r2_rf}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Model Comparison\n",
                "Out of all the models. The decision Tree regressor is giving an r2-score of 96%, it means the model is able to explain 96% of the data. so we will select the decision tree model and save it."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Save The Model\n",
                "Save the model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Saving the best model (Decision Tree)\n",
                "final_model = dt\n",
                "\n",
                "with open('model.save', 'wb') as f:\n",
                "    pickle.dump(final_model, f)\n",
                "\n",
                "with open('transform.save', 'wb') as f:\n",
                "    pickle.dump(scaler, f)\n",
                "\n",
                "print(\"Model (Decision Tree) and Scaler saved.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}